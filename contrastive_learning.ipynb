{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from dataset import ContrastiveDataset\n",
    "from optimizer import LARS\n",
    "from loss import NT_Xent\n",
    "from model import ContrastiveLearningModel\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14b10eba6090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "GPU Name: Tesla P100-PCIE-16GB\n",
      "Total GPU Memory: 16280.69 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "gpu = torch.cuda.get_device_properties(0)  # Assuming you have only one GPU, so index is 0\n",
    "print(f\"GPU Name: {gpu.name}\")\n",
    "print(f\"Total GPU Memory: {gpu.total_memory / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader for self-supervised case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "cifar_test = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_img_array = np.array([np.array(image) for image, _ in cifar_train])\n",
    "test_img_array = np.array([np.array(image) for image, _ in cifar_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ContrastiveDataset(\"train\", train_img_array[:40000])\n",
    "val_dataset = ContrastiveDataset(\"val\", train_img_array[40000:])\n",
    "#test_dataset = ContrastiveDataset(\"test\", test_img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_batch_size = 10\n",
    "num_workers = 0 # means no sub-processes, needed for debugging\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=ssl_batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=ssl_batch_size, shuffle=False, num_workers=num_workers\n",
    ")\n",
    "#test_dataloader = DataLoader(\n",
    " #   test_dataset, batch_size=ssl_batch_size, shuffle=False, num_workers=num_workers\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveLearningModel().to(device)\n",
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "criterion = NT_Xent(batch_size=ssl_batch_size, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/work/jssaethe/tdt05_ssl/SSL_project/optimizer.py:132: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  next_v.mul_(momentum).add_(scaled_lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1, training loss: 1.4723989049494266, validation loss: 1.376311990737915, time: 604.14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m training_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m (x_i, x_j) \u001b[39min\u001b[39;00m train_dataloader: \n\u001b[1;32m     11\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m   x_i, x_j \u001b[39m=\u001b[39m x_i\u001b[39m.\u001b[39mto(device), x_j\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/cluster/work/jssaethe/tdt05_ssl/SSL_project/dataset.py:47\u001b[0m, in \u001b[0;36mContrastiveDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m sample \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[1;32m     46\u001b[0m augmented_sample1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment(torch\u001b[39m.\u001b[39mfrom_numpy(sample))\n\u001b[0;32m---> 47\u001b[0m augmented_sample2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maugment(torch\u001b[39m.\u001b[39;49mfrom_numpy(sample))\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m augmented_sample1, augmented_sample2\n",
      "File \u001b[0;32m/cluster/work/jssaethe/tdt05_ssl/SSL_project/dataset.py:65\u001b[0m, in \u001b[0;36mContrastiveDataset.augment\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform augmentation if this is the training partition\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartition \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(sample)\n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m sample\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torchvision/transforms/transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     59\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 60\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torchvision/transforms/transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     59\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 60\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torchvision/transforms/transforms.py:489\u001b[0m, in \u001b[0;36mRandomApply.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n\u001b[1;32m    488\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m--> 489\u001b[0m     img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m    490\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1184\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m   1183\u001b[0m     \u001b[39melif\u001b[39;00m fn_id \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m hue_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m         img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49madjust_hue(img, hue_factor)\n\u001b[1;32m   1186\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torchvision/transforms/functional.py:846\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    844\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39madjust_hue(img, hue_factor)\n\u001b[0;32m--> 846\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49madjust_hue(img, hue_factor)\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:197\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39muint8:\n\u001b[1;32m    195\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32) \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[0;32m--> 197\u001b[0m img \u001b[39m=\u001b[39m _rgb2hsv(img)\n\u001b[1;32m    198\u001b[0m h, s, v \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39munbind(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m    199\u001b[0m h \u001b[39m=\u001b[39m (h \u001b[39m+\u001b[39m hue_factor) \u001b[39m%\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/maptr/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:352\u001b[0m, in \u001b[0;36m_rgb2hsv\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    349\u001b[0m bc \u001b[39m=\u001b[39m (maxc \u001b[39m-\u001b[39m b) \u001b[39m/\u001b[39m cr_divisor\n\u001b[1;32m    351\u001b[0m hr \u001b[39m=\u001b[39m (maxc \u001b[39m==\u001b[39m r) \u001b[39m*\u001b[39m (bc \u001b[39m-\u001b[39m gc)\n\u001b[0;32m--> 352\u001b[0m hg \u001b[39m=\u001b[39m ((maxc \u001b[39m==\u001b[39;49m g) \u001b[39m&\u001b[39;49m (maxc \u001b[39m!=\u001b[39;49m r)) \u001b[39m*\u001b[39;49m (\u001b[39m2.0\u001b[39;49m \u001b[39m+\u001b[39;49m rc \u001b[39m-\u001b[39;49m bc)\n\u001b[1;32m    353\u001b[0m hb \u001b[39m=\u001b[39m ((maxc \u001b[39m!=\u001b[39m g) \u001b[39m&\u001b[39m (maxc \u001b[39m!=\u001b[39m r)) \u001b[39m*\u001b[39m (\u001b[39m4.0\u001b[39m \u001b[39m+\u001b[39m gc \u001b[39m-\u001b[39m rc)\n\u001b[1;32m    354\u001b[0m h \u001b[39m=\u001b[39m (hr \u001b[39m+\u001b[39m hg \u001b[39m+\u001b[39m hb)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "  start = time.time()\n",
    "  model.train()\n",
    "  training_loss = 0\n",
    "  for (x_i, x_j) in train_dataloader: \n",
    "    optimizer.zero_grad()\n",
    "    x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "\n",
    "    z_i = model(x_i)\n",
    "    z_j = model(x_j)\n",
    "\n",
    "    loss = criterion(z_i, z_j)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    training_loss += loss.item()\n",
    "  \n",
    "  training_loss /= len(train_dataloader)\n",
    "  training_losses.append(training_loss)\n",
    "  \n",
    "  model.eval()\n",
    "  with torch.no_grad(): \n",
    "    validation_loss = 0\n",
    "    for (x_i, x_j) in val_dataloader: \n",
    "      x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "\n",
    "      z_i = model(x_i)\n",
    "      z_j = model(x_j)\n",
    "\n",
    "      loss = criterion(z_i, z_j)\n",
    "      validation_loss += loss.item()\n",
    "\n",
    "    validation_loss /= len(val_dataloader)\n",
    "    validation_losses.append(validation_loss)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "  print(f\"Epoch #{epoch+1}, training loss: {training_loss}, validation loss: {validation_loss}, time: {end - start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1, len(training_losses), len(training_losses))\n",
    "plt.plot(x, training_losses, label=\"Training\")\n",
    "plt.plot(x, validation_losses, label=\"Valdation\")\n",
    "\n",
    "plt.xlabel(\"Num epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Contrastive Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to file\n",
    "model_path = \"models/encoder.pth\" \n",
    "torch.save(model.encoder, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maptr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "668b7188361740d0a5128326eced1d926ea833d7aff85ed8b6757ad1701c0d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
