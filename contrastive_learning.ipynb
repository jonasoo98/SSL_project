{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from dataset import ContrastiveDataset\n",
    "from optimizer import LARS\n",
    "from loss import NT_Xent\n",
    "from model import ContrastiveLearningModel\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15102af0d570>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader for self-supervised case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "cifar_test = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_img_array = np.array([np.array(image) for image, _ in cifar_train])\n",
    "test_img_array = np.array([np.array(image) for image, _ in cifar_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ContrastiveDataset(\"train\", train_img_array[:40000])\n",
    "val_dataset = ContrastiveDataset(\"val\", train_img_array[40000:])\n",
    "test_dataset = ContrastiveDataset(\"test\", test_img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_batch_size = 200\n",
    "num_workers = 0 # means no sub-processes, needed for debugging\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=ssl_batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=ssl_batch_size, shuffle=False, num_workers=num_workers\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=ssl_batch_size, shuffle=False, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveLearningModel().to(device)\n",
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "criterion = NT_Xent(batch_size=ssl_batch_size, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/work/jssaethe/SSL_project/optimizer.py:132: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  next_v.mul_(momentum).add_(scaled_lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1, training loss: 4.97456280708313, validation loss: 4.376198015213013, time: 184.05\n",
      "Epoch #2, training loss: 4.442817897796631, validation loss: 4.183467617034912, time: 186.78\n",
      "Epoch #3, training loss: 4.345142996311187, validation loss: 4.161166410446167, time: 176.69\n",
      "Epoch #4, training loss: 4.304538214206696, validation loss: 4.13193907737732, time: 176.92\n",
      "Epoch #5, training loss: 4.275191602706909, validation loss: 4.109586181640625, time: 176.47\n",
      "Epoch #6, training loss: 4.255799016952515, validation loss: 4.106891975402832, time: 176.79\n",
      "Epoch #7, training loss: 4.24580753326416, validation loss: 4.1015540218353275, time: 176.53\n",
      "Epoch #8, training loss: 4.23561849117279, validation loss: 4.108452081680298, time: 177.94\n",
      "Epoch #9, training loss: 4.227426695823669, validation loss: 4.098489170074463, time: 177.16\n",
      "Epoch #10, training loss: 4.221007130146027, validation loss: 4.092049160003662, time: 175.01\n",
      "Epoch #11, training loss: 4.2136726832389835, validation loss: 4.0888745307922365, time: 175.74\n",
      "Epoch #12, training loss: 4.207433767318726, validation loss: 4.089075374603271, time: 176.21\n",
      "Epoch #13, training loss: 4.201963436603546, validation loss: 4.092022953033447, time: 177.00\n",
      "Epoch #14, training loss: 4.198799676895142, validation loss: 4.084453620910645, time: 176.35\n",
      "Epoch #15, training loss: 4.1931793808937075, validation loss: 4.083379411697388, time: 176.60\n",
      "Epoch #16, training loss: 4.1897385835647585, validation loss: 4.077520380020141, time: 176.73\n",
      "Epoch #17, training loss: 4.186071395874023, validation loss: 4.080934772491455, time: 176.53\n",
      "Epoch #18, training loss: 4.182466516494751, validation loss: 4.082279424667359, time: 178.24\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "  start = time.time()\n",
    "  model.train()\n",
    "  training_loss = 0\n",
    "  for (x_i, x_j) in train_dataloader: \n",
    "    optimizer.zero_grad()\n",
    "    x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "\n",
    "    z_i = model(x_i)\n",
    "    z_j = model(x_j)\n",
    "\n",
    "    loss = criterion(z_i, z_j)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    training_loss += loss.item()\n",
    "  \n",
    "  training_loss /= len(train_dataloader)\n",
    "  training_losses.append(training_loss)\n",
    "  \n",
    "  model.eval()\n",
    "  with torch.no_grad(): \n",
    "    validation_loss = 0\n",
    "    for (x_i, x_j) in val_dataloader: \n",
    "      x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "\n",
    "      z_i = model(x_i)\n",
    "      z_j = model(x_j)\n",
    "\n",
    "      loss = criterion(z_i, z_j)\n",
    "      validation_loss += loss.item()\n",
    "\n",
    "    validation_loss /= len(val_dataloader)\n",
    "    validation_losses.append(validation_loss)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "  print(f\"Epoch #{epoch+1}, training loss: {training_loss}, validation loss: {validation_loss}, time: {end - start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1, len(training_losses), len(training_losses))\n",
    "plt.plot(x, training_losses, label=\"Training\")\n",
    "plt.plot(x, validation_losses, label=\"Valdation\")\n",
    "\n",
    "plt.xlabel(\"Num epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Contrastive Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to file\n",
    "model_path = \"models/encoder.pth\" \n",
    "torch.save(model.encoder, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maptr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "668b7188361740d0a5128326eced1d926ea833d7aff85ed8b6757ad1701c0d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
